
%%%%%%%%%%% - Preamble
\documentclass[11pt]{article}
\usepackage[sans, stdmargin, noindent]{../rajeev}
\usepackage{lastpage}
\usepackage[inline]{asymptote}
\usepackage[normalem]{ulem}
\usepackage{setspace}
\usepackage[backend=biber]{biblatex}
\addbibresource{citations.bib}
\pagestyle{fancy}
\renewcommand*\contentsname{Table of Contents}

\usepackage{amssymb}
\renewcommand{\qedsymbol}{$\blacksquare$}


\usepackage{hyperref}



\rhead{Last Updated: \today}
\lhead{Math 291 Notes}


\setlength{\headheight}{13.6pt}

\begin{document}


\begin{center}
    \Large \textbf{Math 291: Honors Calculus III}
\end{center}
\begin{center}
    \Large Rajeev Atla
\end{center}

\begin{itemize}
\item Class taught by Ian Jauslin (assistant professor)
\item Recitations taught by Yael Davidov (5th year grad student)
\item Textbook is \emph{Multivariable Calculus} by Eric Carlen \cite{carlen}
  \begin{itemize}
  \item Given to us
  \item Author taught class for several years
    \begin{itemize}
    \item Last taught 2019-2020 school year
    \end{itemize}
  \item Last updated August 2019
  \end{itemize}
\item Additional resources
  \begin{itemize}
  \item \emph{Vector Calculus, Linear Algebra, and Differential Forms} by Hubbard and Hubbard \cite{hubbard}
  \end{itemize}
\end{itemize}

\newpage

\doublespacing
\tableofcontents
\singlespacing

\lecture{September 1}

\subsection{Introduction}
Course is about calculus - multivariable calculus.

Single variable calculus is assumed.

The course is very dense and has a lot of material.
As such, some nonessential/easy proofs/examples will be relegated to the textbook or recitations.


\subsection{Why 3D Calc}
The world is 3D.
We need to generalize 1D calc for it to be applicable to a lot of things.
We're gonna be working in an arbitrary number of dimensions.
3 dimensions isn't enough - we often deal with rotation, which adds 3 more angular parameters in addition to 3 for position.
But we don't stop here - we continue onwards to $n$ dimensions.

We focus less on numbers and more on the concepts - this lets us learn linear algebra, which isn't usually introduced in a typical calculus class.

\subsection{Vector Variables and  Cartesian Coordinates}

This section follows 1.1.2 of \cite{carlen}.

\begin{definition}{Functions}
  A function takes an ordered collection of numbers and maps it to another ordered collection of numbers.
\end{definition}

\begin{definition}{Cartesian coordinates}
  Tells you where a particle is using a base point (the origin $O$) and a set of base vectors: $\pars{\bm{e}_1, \dots, \bm{e}_n}$.
  We can follow each base vector a certain distance from the base point to get coordinates that specify direction.
  We can extend this to an arbitrary number of dimensions by increasing the number of base vectors.
\end{definition}

The Pythagorean theorem in multiple dimensions can be used to find distance between these two points.

You can specify a geometric object by defining it as the set of points that satisfy an equation.

\begin{example}{}
  The unit circle is defined as the set of points in 2D that satisfy $x^2 + y^2 = 1$.
\end{example}


\subsection{Parametrization}

\begin{definition}{Parametrization}
  It maps complicated geometric objects to simpler objects (usually intervals) using analytic functions.
\end{definition}

\begin{example}
  
  $\pars{x = \cos{\theta}, y = \sin{\theta}}$ where $\theta \in [-\pi, \pi]$ is a map that maps $\theta$ to a pair of coordinates.
  
  $$
  X\pars{\theta} = \pars{\cos \theta, \sin \theta}
  $$

  This is called a coordinate function.
\end{example}

\begin{example}
  
  The inverse of this map:
  $$
  \theta :=
  \begin{cases}
    \arccos{x} & y \geq 0 \\
    - \arccos{x} & y < 0 \\
  \end{cases}
  $$

  The inverse cosine is only defined over the interval $\theta \in (-\pi, \pi]$.
\end{example}


\begin{example}

  $$x^2 + y^2 = r^2$$

  We assume that $r>0$.
  With the scaling variable $r$ we can transform the parametrization of the unit circle (just above) into an appropriate parametrization for a circle of an arbitray radius.

  
  $$
  \theta \mapsto \pars{r \cos \theta, r \sin \theta}
  $$

  We can also define the inverse: the coordinate function.

  $$
  \pars{x, y} \mapsto \theta := \begin{cases}
    \arccos \pars{x/\sqrt{x^2 + y^2}} & y \geq 0\\
    -\arccos \pars{x/\sqrt{x^2 + y^2}} & y < 0 \\
    \end{cases}
  $$
  
\end{example}

\begin{example}

  We parametrize the unit sphere.

  $$x^2 + y^2 + z^2 = 1$$

We need two variables since its a circle (which is 1D - a line shaped into a circle) rotated.

$$z = \cos \phi,\ \phi \in \brak{0, \pi}$$

This implies $$x^2 +y^2 = \sin^2 \phi$$

$x$ and $y$ lie on a circle of radius $\sin \phi$.

It's pretty easy to parametrize this

$$x = \sin\phi \cos \theta$$

$$y = \sin\phi \sin \theta$$

$$\theta \in \brak{-\pi, \pi}$$

Here, $\phi$ is the angle between the point on the sphere and the z-axis.
At the North and South poles, $\theta$ doesn't matter anymore.
The parametrization function only holds on $\phi \neq 0,  \pi$.
\end{example}


Parametrization can be used to simplify all sorts of computationally tedious problems.

\begin{example}
  
  We compute the tangent plane to a sphere.

  In 1D calculus, the tangent line to curve is the best local approximation of the curve.
  For example, at a point $x_0$,  $y = \cos \pars{x_0 + t} \approx \cos \pars{x_0} - t \sin \pars{x_0}$.

  To make this easier suppose we have a fixed point: $\pars{x_0, y_0, z_0} = \pars{\frac{1}{2}, \frac{1}{2}, \frac{1}{\sqrt{2}}}$ or $\phi = \frac{\pi}{4}, \theta = \frac{\pi}{4}$.

  We can parametrize the shift in the angles with $\phi = \frac{\pi}{4} + s, \theta = \frac{\pi}{4} + t$. For $s, t \ll 1$ (very small changes), we can write

  \begin{align*}
    x &= \sin \pars{ \frac{\pi}{4} + s} \cos \pars{ \frac{\pi}{4} + t} \\
      & \approx \pars{\sin \pars{\frac{\pi}{4}} + s \cos \pars{\frac{\pi}{4}}} \pars{\cos \pars{\frac{\pi}{4}} - t \sin \pars{\frac{\pi}{4}}} \\
      &= \frac{1}{2} \pars{1+s} \pars{1-t} \\
    y &= \sin \pars{\frac{\pi}{4} + s} \sin \pars{ \frac{\pi}{4} + t} \\
      & \approx \pars{\sin \pars{\frac{\pi}{4}} + s \cos \pars{\frac{\pi}{4}}}  \pars{\sin \pars{\frac{\pi}{4}} + t \cos \pars{\frac{\pi}{4}}} \\
      &= \frac{1}{2} \pars{1+s} \pars{1+t} \\
    z &= \cos \pars{\frac{\pi}{4} + s} \\
      & \approx \frac{1}{\sqrt{2}} - \frac{s}{\sqrt{2}} \\
  \end{align*}

  Note: taking a Taylor expansion and taking a derivative have a lot of things in common.
\end{example}





\subsection{Vector Space $\RR^n$}

\begin{definition}{Vectors}
  A vector is an ordered collection of numbers in $\RR$.

  $$\bm{x} \in \RR^m : m \in \NN_* : \bm{x} \equiv (x_1, \dots, x_m)$$
\end{definition}





This means $\RR^n$ is a vector space.




\begin{definition}{Vector Space}
You can multiply a member of the space by a scalar and still be in the vector space.
You can also add members of the same vector space and the resultant vector will still be in the vector space.

Distributive property: if you multiply by a scalar $\alpha$ the resultant vector will still be in the vector space.

Commutative property: the order in which you add vectors of a vector space doesn't matter; the sum will still be the same member of the vector space.

Associative property: the order in which you group addition doesn't matter.
\end{definition}


\begin{definition}{Span}
  $\spn(V)$

  The span of a subspace is all possible linear combinations of vectors in $V$ - also a vector space.
\end{definition}





\begin{theorem}{Span Identity Theorem}
  Let $W = \spn(V)$, where $V \subset \RRN$.
  Then $\spn(W) = W$.
  \begin{proof}
    Since $W = \spn V$, $V \subseteq W$.
    Therefore, $\spn V \subseteq \spn W$.
    Pick indices $i, j, k \in \NN $ such that $i + j = k$ and scalars $s_1, \cdots, s_k \in \RR$.
    In addition, we also choose vectors $\bm{w}_1, \bm{w}_2 \in W$ and $\bm{v}_1, \cdots, \bm{v}_k \in V$, where
    \begin{align*}
      \bm{w}_1 &= \sum \limits_{n=0}^i s_n \bm{v}_n\\
      \bm{w}_2 &= \sum \limits_{n=i+1}^{k} s_n \bm{v}_n\\
    \end{align*}
    
    We attempt to write another distinct vector $\bm{b}$ (a generalized member of $\spn W$) that is a linear combination of these two vectors using scalars $t_1, t_2 \in \RR$.
    $$
    \bm{b} = t_1 \bm{w}_1 + t_2 \bm{w}_2 = \sum \limits_{n=1}^{n=i} \pars{t_1 s_n} \bm{v}_n  + \sum \limits_{n=i+1}^{n=k} \pars{t_2 s_n} \bm{v}_n
    $$

    However, we see that this is simply a linear combination of $\bm{v}_1, \cdots, \bm{v}_k$.
    Therefore, $\bm{b} \in \spn V = W$.
    Since $\bm{b}$ is really a formalized way to represent a member of $\spn W$, we can see that. Therefore, $\spn W \subseteq \spn V$.
    However since we've already established that $\spn V \subseteq \spn W$, the only option is for $\spn V = \spn W$ or $\spn W = W$.

    This proves it for the case of 2 vectors.
    We need to generalize it to an arbitrary number of vectors, after which we can complete the proof using induction.
  \end{proof}
\end{theorem}





\begin{definition}{Standard Bases}
  Standard bases are the vectors $\bm{e}_1, \cdots, \bm{e}_n \in \RRN$, where

  \begin{align*}
    \bm{e}_1 &= \pars{1, 0, \cdots, 0} \\
    & \vdots \\
    \bm{e}_n &= \pars{0, 0, \cdots, 1} \\
  \end{align*}

  It's sometimes useful to note that bases are the shortest way to write a span.
\end{definition}


\begin{theorem}{Standard Bases}
  $\spn (e_1, \cdots, e_n) = \RR^m$
  
  All vectors in $\RR^m$ are linear combinations of the standard bases
  \begin{proof}
    Pick some scalars $\pars{t_1, \cdots t_m} \in \RR$.
    It's easy to see that for a sufficient choice for $t_i$, we can create any vector $\bm{x} \in \RRM$.
    $$
    \bm{x} = \sum t_i \bm{e}_i
    $$
  \end{proof}
\end{theorem}








\recitation{September 2}

\begin{example}
  
  Suppose $V = \set{\pars{0, 0, 0}}$.
  Then $\spn V$ only has one element since you can't make anything but $\bm{0}$ out of the zero-vector.
\end{example}

\begin{example}

  Let $V = \set{\pars{1, 0, 0}}$.
  $$
  \spn V = \set{\pars{t, 0, 0} : t \in \RR}
  $$
\end{example}

\begin{example}
  
  Let $\bm{v}_1 = \pars{1, 2, -3}$ and $\bm{v}_2 = \pars{1, -2, 1}$.
  Can we find an equation that is satisfied by a vector $\bm{v} = \pars{x, y, z}$ if and only if $\bm{v} \in \spn \set{\bm{v}_1, \bm{v}_2}$?

  Letting $s,t \in \RR$,
  \begin{align*}
    \bm{v} &\in \spn \set{\bm{v}_1, \bm{v}_2} \\
    \bm{v} &= s \bm{v}_1 + t \bm{v}_2 \\
           &= s \pars{1,2,-3} + t \pars{1, -2, 1} \\
           &= \pars{s+t, 2s-2t, -3s+t} \\
    x &= s+t \\
    y &= 2s-2t \\
    z &= -3s+t \\
  \end{align*}

  We solve the equations for $x$ and $y$ to see that
  \begin{align*}
    s &= \frac{1}{4} \pars{2x+y} \\
    t &= \frac{1}{4} \pars{2x+y} \\
  \end{align*}

  Resubstituting into the equation for $z$, we see that
  \begin{align*}
    z &= -x-y \\
    x+y+z &= 0 \\
  \end{align*}

  So $\bm{v} = \pars{x, y, z} \in \spn \set{\bm{v}_1, \bm{v}_2} \leftrightarrow z+x+y=0$
\end{example}

\begin{example}
  
  Let $\bm{v}_3 = \pars{-2, 1, 1}$.
  Can you describe $\spn \set{\bm{v}_1, \bm{v}_2, \bm{v}_3}$? (vectors $\bm{v}_1, \bm{v}_2$ from previous example)?

  Let $r, s, t \in \RR$.
  Suppose $\bm{u} \in \spn \set{\bm{v}_1, \bm{v}_2, \bm{v}_3}$.
  Then,

  \begin{align*}
    \bm{u} &= r \bm{v}_1 + s \bm{v}_2 + t \bm{v}_3 \\
    \pars{x, y, z} &= \pars{r+s-2t, 2r-2s+t, -3r+s+t} \\
  \end{align*}
  Adding all the equations together, we see that $x+y+z=0$, once again.
  This means that the span hasn't changed.
\end{example}

\begin{example}
  
  Let $\bm{u} = \pars{1,2}, \bm{v} = \pars{-2, -4}, \bm{w} = \pars{7,14}$ be three vectors in $\RR^2$.
  Show that $\bm{w} \in \spn \set{\bm{u}, \bm{v}}$.

  We can write $\bm{w} = s \bm{u} + t \bm{v}$.
  There are many such pairs of $(s,t)$, which are computationally trivial to solve for.

  Another, quicker, approach is to see that $\bm{u} \in \spn \bm{v}$ and vice versa.
  Therefore, $\spn \set{\bm{u}, \bm{v}} = \spn \bm{v} $.
\end{example}

\begin{example}

  Let $\bm{u}_1, \bm{u}_2$ be orthogonal unit vectors.
  Define
  $$
  \bm{v} := a \bm{u}_1 + b \bm{u}_2
  $$
  Find $\bm{v} \cdot \bm{u}_1$.

  \begin{align*}
    \bm{v} \cdot \bm{u}_1 &= a \bm{u}_1 \cdot \bm{u}_1 + b \bm{u}_2 \cdot \bm{u}_1 \\
                          &= a + b \cdot 0 \\
                          &= a \\
  \end{align*}
\end{example}


\begin{definition}{Set Equality}
  If you want to prove that $A = B$, then it is sufficient to prove that both $A \subseteq B$ and $B \subseteq A$.
\end{definition}



\lecture{September 8}

\subsection{Dot Products}
This section follows 1.1.5 of \cite{carlen}.

The fundamental property of geometry is distance.

\begin{definition}{Metric}
      A metric $\rho$ takes a pair of points and returns a non-negative real number.

      Properties:

      \begin{align*}
        \rho \pars{x, y} &= \rho \pars{y, x} \\
        \rho \pars{x, y} &= 0 \leftrightarrow x = y \\
        \rho \pars{x, z} & \leq \rho \pars{x, y} + \rho \pars{y, z} \\
      \end{align*}

      The last property is satisfied by the Euclidean norm using the triangular inequality.
\end{definition}

\begin{definition}{Euclidean Norm}
  \cite{carlen} calls this Euclidean distance.

  Suppose we have a vector $\bm{x} \in \RR^n$.
  We define the Euclidean norm of that vector $\norm{\bm{x}}$ by
  $$
  \norm{\bm{x}} = \sqrt{\sum \limits_{j = 1}^n x_i^2}
  $$
\end{definition}

\begin{example}

  The norm in $\RR^2$ is
  $$
  \sqrt{x^2 + y^2}
  $$
\end{example}

\begin{example}

  The norm in $\RR^3$ is
  $$
  \sqrt{x^2 + y^2 + z^2}
  $$
\end{example}

The norm of the difference between two vectors is their Euclidean distance.

$$
\norm{\bm{x} - \bm{y}} = \rho_E \pars{x, y}
$$

Some properties of the Euclidean distance:

Another cool property is that for any $t \in \RR$,
$$\norm{t \bm{x}} = \abs{t} \norm{\bm{x}}$$

\begin{definition}{Unit(ary) vector}
  

  You can obtain a unit vector by dividing a vector by its norm.
  Let $\bm{x} \in \RR^n$.
  The unit vector $\bm{u}$ is

  $$
  \bm{u} := \frac{\bm{x}}{\norm{\bm{x}}}
  $$

  By definition (and it's also pretty easy to see this), $\norm{\bm{u}} = 1$ and $\bm{u}$ and $\bm{x}$ point in the same direction.
\end{definition}



\begin{definition}{Dot Product AKA Scalar Product}
  Let $\bm{x}, \bm{y} \in \RR^n$.

  The dot product of these two vectors is defined as

  $$
  \bm{x} \cdot \bm{y} = \sum \limits_{i=1}^{i=n} x_i y_i
  $$
  
\end{definition}

The norm is the square root of vector dotted with itself:
$$
\norm{\bm{x}} = \sqrt{\bm{x} \cdot \bm{x}}
$$

This is analogous to absolute value:
$$
\abs{x} = \sqrt{x \cdot x}
$$


The dot product is distrbutive:

$$
\bm{x} \cdot \pars{\bm{y} + \bm{z}} = \bm{x} \cdot \bm{y} + \bm{x} \cdot \bm{z}
$$



\begin{align*}
  \norm{\bm{x} - \bm{y}}^2 &= \pars{\bm{x} - \bm{y}} \cdot \pars{\bm{x} - \bm{y}} \\
                           &= \bm{x} \cdot \bm{x} + \bm{y} \cdot \bm{y} - 2 \bm{x} \cdot \bm{y} \\
                           &= \norm{\bm{x}}^2 + \norm{\bm{y}}^2 - 2 \bm{x} \cdot \bm{y} \\
\end{align*}


\begin{center}
  \begin{asy}
    size(5cm);
    draw((25, 30) -- (0, 0), arrow = Arrow(HookHead));
    draw((25, 30) -- (50, 25), arrow = Arrow(HookHead));
    draw((50, 25) -- (0, 0), arrow = Arrow(HookHead));

    label("$\bm{x}$", (10, 15), NE);
    label("$\bm{y}$", (39, 27.5), NW);
    label("$\bm{x} - \bm{y}$", (25, 12.5), SE);
    label("$\theta$", (25, 30), S);
  \end{asy}
\end{center}



This bears a striking similarity to the law of cosines.
We see in the triangle above that
$$
\norm{\bm{x} - \bm{y}}^2 = \norm{\bm{x}}^2 + \norm{\bm{y}}^2 - 2 \norm{\bm{x}} \norm{\bm{x}} \cos \theta
$$


This is how you see $\bm{x} \cdot \bm{y} = \norm{x} \norm{y} \cos \theta$.

\begin{definition}{Angle Between Vectors}
  
  The angle $\theta$ is defined using the dot product:

  $$
  \theta = \arccos \frac{\bm{x} \cdot \bm{y}}{\norm{\bm{x}} \norm{\bm{y}}}
  $$
  
  Note: we need to prove that this quotient is in the domain of the inverse cosine function, which we do using the Cauchy-Schwarz Inequality (below).
\end{definition}

\begin{theorem}{Cauchy-Schwarz Inequality}
  $$\abs{\bm{x} \cdot \bm{y}} \leq \norm{\bm{x}} \norm{\bm{y}}$$

  \begin{proof}
    
    Define two unitary versions of x and y, $\bm{a}$ and $\bm{b}$, respectively.
    
    \begin{align*}
      \bm{a} & := \frac{\bm{x}}{\norm{\bm{x}}} \\
      \bm{b} & := \frac{\bm{y}}{\norm{\bm{y}}} \\
    \end{align*}

    We have

    \begin{align*}
      \norm{\bm{a} - \bm{b}}^2 &= \norm{\bm{a}}^2 + \norm{\bm{b}}^2 - 2 \bm{a} \cdot \bm{b} \\
      &= 2 \pars{1 - \bm{a} \cdot \bm{b}} \\
    \end{align*}
    Since $\norm{\bm{a} - \bm{b}}^2 \geq 0$, that means $\bm{a} \cdot \bm{b} \leq 1$.
    Similarly,
    
    $$
    \norm{\bm{a} + \bm{b}}^2 = 2 \pars{1 + \bm{a} \cdot \bm{b}}
    $$

    Therefore, $\bm{a} \cdot \bm{b} \geq -1$.
    Putting these two inequalities together,
    
    $$
    -1 \leq \bm{a} \cdot \bm{b} \leq 1
    $$

    By the definition of $\bm{a}, \bm{b}$, this proves the inequality.
  \end{proof}
\end{theorem}

\begin{theorem}{Triangle Inequality}
  For any three vectors $\bm{x}, \bm{y}, \bm{z} \in \RR^n$,

  $$
  \norm{\bm{x} - \bm{z}} \geq \norm{\bm{x} - \bm{y}} + \norm{\bm{y} - \bm{z}} 
  $$

  \begin{proof}
    Define:

    \begin{align*}
      \bm{a} & := \bm{x} - \bm{y} \\
      \bm{b} & := \bm{z} - \bm{y} \\
    \end{align*}

    We then have

    \begin{align*}
      \norm{\bm{x} - \bm{z}}^2 &= \norm{\bm{a} - \bm{b}}^2 \\
                               &= \norm{\bm{a}}^2 + \norm{\bm{b}}^2 - 2 \bm{a} \cdot \bm{b} \\
      \text{(Cauchy-Schwartz)\,}         & \leq \norm{\bm{a}}^2 + \norm{\bm{b}}^2 + 2 \bm{a} \cdot \bm{b} \\
                               &= \pars{\norm{\bm{a}} + \norm{\bm{b}}}^2 \\
      \norm{\bm{x} - \bm{z}} & \leq \norm{\bm{a}} + \norm{\bm{b}} \\
    \end{align*}
    Using the definition of $\bm{a}, \bm{b}$ finishes the proof.
  \end{proof}
\end{theorem}

\subsection{Cross Products in $\RR^3$}


\begin{center}
  \begin{asy}
    size(5cm);
    draw((0, 0) -- (15, 15), arrow = Arrow(HookHead));
    draw((0, 0) -- (15, -15), arrow = Arrow(HookHead));
    draw((15, 15) -- (30, 0), arrow = Arrow(HookHead), dashed);
    draw((15, -15) -- (30, 0), arrow = Arrow(HookHead), dashed);
    
    label("$\bm{a}$", (7.5, 7.5), NW);
    label("$\bm{b}$", (7.5, -7.5), SW);
    label("$\theta$", (0, 0), E);
  \end{asy}
\end{center}

The area of parallelogram is (from basic geometry)
$$A = \norm{\bm{a}} \norm{\bm{b}} \sin \theta$$

\begin{align*}
  A^2 &= \norm{\bm{a}}^2 \norm{\bm{b}}^2 \sin^2 \theta \\
      &= \norm{\bm{a}}^2 \norm{\bm{b}}^2 \pars{1 - \cos^2 \theta} \\
      &= \norm{\bm{a}}^2 \norm{\bm{b}}^2 - \pars{\bm{a} \cdot \bm{b}}^2 \\
\end{align*}

Splitting $\bm{a}$ and $\bm{b}$ into components, we see that

\begin{align*}
  A^2 &= \pars{a_2 b_3 - a_3 b_2}^2 + \pars{a_3 b_1 - a_1 b_3}^2 + \pars{a_1 b_2 - a_2 b_1}^2 \\
\end{align*}

We define a useful operation to get this quantity, which looks like the norm of a vector: the cross product.

\begin{definition}{Cross Product}
  
  $$
  \bm{a} \times \bm{b} = \pars{a_2 b_3 - a_3 b_2} \bm{e}_1 + \pars{a_3 b_1 - a_1 b_3} \bm{e}_2 + \pars{a_1 b_2 - a_2 b_1} \bm{e}_3
  $$

  A more convenient way to remember this is
  $$
  \bm{a} \times \bm{b} =
  \begin{vmatrix}
    \bm{e}_1 & \bm{e}_2 & \bm{e}_3 \\
    a_1 & a_2 & a_3 \\
    b_1 & b_2 & b_3 \\
  \end{vmatrix}
  $$
\end{definition}

By construction,
$$
A = \norm{\bm{a} \times \bm{b}}
$$


It's easy to see with some algebra that the cross product is anticommutative, so
$$
\bm{b} \times \bm{a} = - \bm{a} \times \bm{b}
$$


\begin{example}

  It's easy to see that
  
  $$\bm{a} \times \bm{a} = 0$$
  
  It follows that for all $t \in \RR$
  
  $$
  \bm{a} \times \pars{t \bm{a}} = 0 
  $$

  This means that when parallel or anti-parallel vectors are crossed, the result is always 0.
\end{example}



\begin{theorem}
  
  $$
  (\bm{a} \times \bm{b}) \cdot \bm{c} = (\bm{b} \times \bm{c}) \cdot \bm{a} = (\bm{c} \times \bm{a} ) \cdot \bm{b}
  $$
  Note the cylic permutations of $\pars{\bm{a}, \bm{b}, \bm{c}}$.
  If acylic permutations are introduced, then there's a negative sign added.
  
  \begin{proof}
    Trivial by algebraic expansion.
  \end{proof}
  
\end{theorem}

\begin{example}

  $$ (\bm{a} \times \bm{b}) \cdot \bm{a} = \pars{ \bm{a} \times \bm{a}} \cdot \bm{b} = 0$$
  This means $\bm{a} \times \bm{b}$ is orthogonal to $\bm{a}$.
  
\end{example}

Cross products are less relevant in higher dimensions:
\begin{itemize}
\item In 4D, the cross product is a plane
\item In 5D, the cross product is a sphere
\end{itemize}



\begin{theorem}{Lagrange's Identity}
  
  $$
  \bm{a} \times \pars{\bm{b} \times \bm{c} } = \pars{\bm{a} \cdot \bm{c}} \bm{b} - \pars{\bm{a} \cdot \bm{b}} \bm{c}  $$

    

  \begin{proof}
    
  \end{proof}
\end{theorem}

\subsection{Lines and Planes in $\RR^3$}

\subsubsection{Lines}

\begin{definition}{Line}
  A line is defined as the set of points $\bm{x}$ that satisfy
  
  $$
  \bm{a} \times ( \bm{x} - \bm{x}_0) = 0
  $$

  Parametrization:
  $$\bm{x}(t) = \bm{x}_0 + t \bm{a}$$

  It's easy to see that this satisfies the definition of a line.
\end{definition}




\subsubsection{Planes}
(insert airplane joke here)


\begin{definition}{Planes}
  A plane is the set of points $\bm{x}$ such that
  
  $$(\bm{x} - \bm{x}_0) \cdot \bm{a} = 0$$

  Parametrization:
  $$\bm{x} \pars{s, t} = \bm{x}_0 + s \bm{v}_0 + t \bm{v}_1$$

  Here, $\bm{v}_0, \bm{v}_1$ are vectors in the plane that they help define.
  
\end{definition}







\subsection{Orthonormal Bases}

\begin{definition}{Orthonormal Bases}
  Define the unit vectors $\bm{u}_1, \bm{u}_2, \bm{u}_3$.
  We define an orthonormal basis $\set{\bm{u}_1, \bm{u}_2, \bm{u}_3}$ by assigning the unit vectors the following properties:
  \begin{align*}
    \bm{u}_i \cdot \bm{u}_j &= 0\, \forall i \neq j \\
    \bm{u}_i \cdot \bm{u}_j &= 1\, \forall i = j \\
  \end{align*}
\end{definition}

For the following two definitions, we work with the orthonormal basis
$$\set{\bm{u}_1, \bm{u}_2, \bm{u}_3}$$

\begin{definition}{Right-Handed Orthonormal Bases}
  $$
  \bm{u}_1 \times \bm{u}_2 = \bm{u}_3
  $$
\end{definition}

\begin{definition}{Left-Handed Orthonormal Bases}
  $$
  \bm{u}_1 \times \bm{u}_2 = - \bm{u}_3
  $$
\end{definition}


\recitation{September 9}

\lecture{September 13}

1.1.6, 1.1.7, 1.1.8, 1.2.3

\lecture{September 15}

\recitation{September 16}

\lecture{September 20}

\lecture{September 22}

\recitation{September 23}

\lecture{September 27}

\lecture{September 29}

\recitation{September 30}

\lecture{October 4}

\lecture{October 6}

\recitation{October 7}

\lecture{October 11}

\lecture{October 13}

\recitation{October 14}

\lecture{October 18}

\lecture{October 20}

\recitation{October 21}

\lecture{October 25}

\lecture{October 27}

\recitation{October 28}

\lecture{November 1}

\lecture{November 3}

\recitation{November 4}

\lecture{November 8}

\lecture{November 10}

\recitation{November 11}

\lecture{November 15}

\lecture{November 17}

\recitation{November 18}

\lecture{November 22}

\lecture{November 29}

\lecture{December 1}

\recitation{December 2}

\lecture{December 6}

\lecture{December 8}

\recitation{December 9}
\lecture{December 13}



\newpage
\printbibliography


\end{document}